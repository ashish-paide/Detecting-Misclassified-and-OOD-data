{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as sk\n",
    "import re\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler , TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, BertConfig , BertForSequenceClassification , AdamW\n",
    "from tqdm import tqdm\n",
    "from torch import cuda\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='./data/imdb.train'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\W+', ' ', line).strip().lower()  # perhaps don't make words lowercase?\n",
    "            x.append(line[:-1])\n",
    "            x[-1] = ' '.join(word for word in x[-1].split() if word not in stop_words)\n",
    "            y.append(line[-1])\n",
    "    return x, np.array(y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "VALID_BATCH_SIZE = 100\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your sentiment analysis dataset and preprocess it\n",
    "X_train_texts, Y_train = load_data('./data/IMDB.train')\n",
    "max_length = 128  # Maximum sequence length\n",
    "X_train_texts, X_test_texts, Y_train, Y_test = train_test_split(X_train_texts, Y_train, test_size=0.2, random_state=42)\n",
    "# Tokenize input texts\n",
    "tokenized_texts = tokenizer(X_train_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "labels = torch.tensor(Y_train, dtype=torch.long)\n",
    "test_labels = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = TensorDataset(tokenized_texts.input_ids, tokenized_texts.attention_mask, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "X_val_texts , Y_val = load_data('./data/IMDB.dev')\n",
    "max_length = 128\n",
    "tokenized_val_texts = tokenizer(X_val_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "val_labels = torch.tensor(Y_val , dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(tokenized_val_texts.input_ids, tokenized_val_texts.attention_mask, val_labels)\n",
    "val_loader =  DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 160/160 [3:26:56<00:00, 77.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   2%|▏         | 3/160 [05:39<4:55:56, 113.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 14\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ashish Paide\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ashish Paide\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_loss = 1e7\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Validate the model\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss.item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "    # Check if this epoch's model is the best so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\"updated\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model\n",
    "        best_model_path = f'best_xlnet_model.pt'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [11:13<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "saved_model_path = 'best_xlnet_model.pt'  # Replace X with the epoch number\n",
    "\n",
    "# Load the saved model's state dictionary\n",
    "state_dict = torch.load(saved_model_path)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Example data for test phase\n",
    "# cr_test_data, cr_test_labels = load_data('./data/.test')\n",
    "# max_length = 200  # Adjust according to your task\n",
    "\n",
    "# X_test_texts = cr_test_data\n",
    "# Y_test = cr_test_labels  # Labels\n",
    "\n",
    "tokenized_texts = tokenizer(X_test_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "# Convert labels to tensors\n",
    "test_labels = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(tokenized_texts.input_ids, tokenized_texts.attention_mask, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Step 4: Evaluate the BERT model\n",
    "model.eval()\n",
    "predictions = []\n",
    "targets = []\n",
    "logits_all, logits_right, logits_wrong = [], [], []\n",
    "probs_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        predictions.extend(probs[:, 1].cpu().numpy())  # Assuming binary classification, we consider the probability of the positive class\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        probs_all.append(probs)\n",
    "        logits_all.append(logits)\n",
    "        logits_right.append(logits[torch.argmax(logits, dim=1) == labels])\n",
    "        logits_wrong.append(logits[torch.argmax(logits, dim=1) != labels])\n",
    " \n",
    "logits_all = torch.cat(logits_all, dim=0)\n",
    "logits_right = torch.cat(logits_right, dim=0)\n",
    "logits_wrong = torch.cat(logits_wrong, dim=0)\n",
    "\n",
    "# Calculate KL divergence for all predictions\n",
    "s_all = torch.nn.functional.softmax(logits_all, dim=1)\n",
    "kl_all = torch.log(torch.tensor(2.)) + torch.sum(s_all * torch.log(torch.abs(s_all) + 1e-10), dim=1, keepdim=True)\n",
    "\n",
    "# Calculate KL divergence for right predictions\n",
    "s_right = torch.nn.functional.softmax(logits_right, dim=1)\n",
    "kl_right = torch.log(torch.tensor(2.)) + torch.sum(s_right * torch.log(torch.abs(s_right) + 1e-10), dim=1, keepdim=True)\n",
    "\n",
    "# Calculate KL divergence for wrong predictions\n",
    "s_wrong = torch.nn.functional.softmax(logits_wrong, dim=1)\n",
    "kl_wrong = torch.log(torch.tensor(2.)) + torch.sum(s_wrong * torch.log(torch.abs(s_wrong) + 1e-10), dim=1, keepdim=True)\n",
    "\n",
    "# Calculate error rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.softmax(logits_all, dim=1)\n",
    "s_prob = torch.max(s, dim=1, keepdim=True)[0]\n",
    "# Calculate KL divergence for all logits\n",
    "kl_a = torch.log(torch.tensor(2.)) + torch.sum(s * torch.log(torch.abs(s) + 1e-10), dim=1, keepdim=True)\n",
    "m_all, v_all = torch.mean(kl_all), torch.var(kl_all)\n",
    "        \n",
    "# Separate logits for correct and wrong predictions\n",
    "predicted_labels = np.round(predictions)  # Round probabilities to get predicted labels\n",
    "# Get indices of correct and wrong predictions\n",
    "correct_indices = np.where(predicted_labels == targets)[0]  # Indices where predictions match targets\n",
    "wrong_indices = np.where(predicted_labels != targets)[0]   # Indices where predictions do not match targets\n",
    "\n",
    "# Separate logits for correct and wrong predictions\n",
    "logits_correct = logits_all[correct_indices]\n",
    "logits_wrong = logits_all[wrong_indices]\n",
    "        \n",
    "# Compute softmax probabilities for correct and wrong predictions\n",
    "s_right = torch.softmax(logits_right, dim=1)\n",
    "s_wrong = torch.softmax(logits_wrong, dim=1)\n",
    "        \n",
    "# Calculate maximum probability for correct and wrong predictions\n",
    "s_rp = torch.max(s_right, dim=1, keepdim=True)[0]\n",
    "s_wp = torch.max(s_wrong, dim=1, keepdim=True)[0]\n",
    "        \n",
    "# Calculate KL divergence for correct and wrong predictions\n",
    "kl_r = torch.log(torch.tensor(2.)) + torch.sum(s_right * torch.log(torch.abs(s_right) + 1e-10), dim=1, keepdim=True)\n",
    "kl_w = torch.log(torch.tensor(2.)) + torch.sum(s_wrong * torch.log(torch.abs(s_wrong) + 1e-10), dim=1, keepdim=True)\n",
    "        \n",
    "# Calculate mean and variance of KL divergence for correct and wrong predictions\n",
    "m_right, v_right = torch.mean(kl_right), torch.var(kl_right)\n",
    "m_wrong, v_wrong = torch.mean(kl_wrong), torch.var(kl_wrong)\n",
    "err = 100 -( sk.accuracy_score(targets, np.round(predictions)))* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.075\n"
     ]
    }
   ],
   "source": [
    "print(sk.accuracy_score(targets, np.round(predictions)) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9904096, 0.9871313, 0.12312431, 0.9245373, 0.9411246, 0.96439856, 0.99012494, 0.011436045, 0.06912318, 0.99209857, 0.7165528, 0.8890971, 0.9721719, 0.98373705, 0.9886829, 0.014101954, 0.0075078844, 0.07464361, 0.028823663, 0.9888466, 0.43473846, 0.9844811, 0.9920597, 0.9883301, 0.09401244, 0.8964868, 0.008746008, 0.14255866, 0.77718663, 0.70298046, 0.9845313, 0.027017178, 0.063265525, 0.9814036, 0.9416032, 0.8689265, 0.25723317, 0.07978328, 0.047431204, 0.9908805, 0.97958976, 0.03350554, 0.9916318, 0.06949036, 0.98898554, 0.9864564, 0.9848722, 0.98714983, 0.9741571, 0.046377137, 0.008891658, 0.9922167, 0.2398995, 0.311526, 0.01468832, 0.98303425, 0.9780913, 0.973254, 0.90379214, 0.07860539, 0.04833122, 0.013708252, 0.011417397, 0.3976177, 0.0118545, 0.5974325, 0.04400099, 0.9398643, 0.9872383, 0.06505074, 0.9898896, 0.9915165, 0.98916376, 0.9889137, 0.99154633, 0.04789002, 0.15827553, 0.98735064, 0.1658706, 0.015099721, 0.008630986, 0.06476465, 0.9846955, 0.021910746, 0.024138564, 0.96076465, 0.98942906, 0.9541541, 0.018631106, 0.80348843, 0.025269743, 0.9905455, 0.014557371, 0.055742495, 0.33523604, 0.008160315, 0.013406913, 0.45233268, 0.011965134, 0.00910339, 0.99198675, 0.30283907, 0.99125916, 0.3997659, 0.15642917, 0.68613803, 0.9863006, 0.9816535, 0.4463603, 0.9601407, 0.41234267, 0.02513619, 0.022361856, 0.017390652, 0.98234946, 0.030971153, 0.49150923, 0.016021268, 0.98590106, 0.9823073, 0.9851285, 0.10311034, 0.088566825, 0.019276105, 0.9750143, 0.9252902, 0.89948046, 0.97617006, 0.24159, 0.03057518, 0.91693336, 0.10507915, 0.98796797, 0.08314473, 0.9917408, 0.8076395, 0.9927811, 0.9366395, 0.09325349, 0.05941623, 0.029153252, 0.99040985, 0.9905782, 0.9823079, 0.47843692, 0.9262863, 0.020874895, 0.020056767, 0.9908669, 0.5490288, 0.009942492, 0.50760895, 0.30381185, 0.98682505, 0.9896661, 0.9782104, 0.017214537, 0.041941978, 0.022906933, 0.025071042, 0.9907602, 0.9844929, 0.9909843, 0.065055035, 0.1675293, 0.11437739, 0.22549766, 0.99127495, 0.21967362, 0.745315, 0.030102134, 0.9908791, 0.76067555, 0.013884172, 0.32631594, 0.9920129, 0.15478516, 0.4635252, 0.027348397, 0.024370838, 0.64068973, 0.9873241, 0.04404926, 0.988247, 0.0067041744, 0.16572814, 0.9759035, 0.01819942, 0.016132228, 0.86479175, 0.055550024, 0.6101842, 0.9878637, 0.8554096, 0.9893343, 0.65903896, 0.7820964, 0.9901426, 0.99134034, 0.35860938, 0.9825067, 0.3298837, 0.34438837, 0.9899184, 0.0072128163, 0.25516376, 0.8489318, 0.9913189, 0.010219661, 0.016258528, 0.976223, 0.986889, 0.987301, 0.012252668, 0.98376447, 0.9279249, 0.016147332, 0.3733618, 0.053414878, 0.83062583, 0.9914996, 0.020001367, 0.7921257, 0.9825204, 0.9321876, 0.98524123, 0.93432456, 0.9088553, 0.9877806, 0.024808511, 0.014661636, 0.1575181, 0.990777, 0.09932435, 0.011049674, 0.009519855, 0.9801721, 0.97840446, 0.023188185, 0.023830479, 0.9912723, 0.013717984, 0.98562837, 0.8174396, 0.9916543, 0.441004, 0.17404014, 0.9906358, 0.0120235, 0.98015517, 0.9893282, 0.38668668, 0.928989, 0.027618902, 0.11849931, 0.096003845, 0.3866125, 0.9866458, 0.9770175, 0.9900851, 0.98739713, 0.015086014, 0.9815679, 0.9845339, 0.89859354, 0.9865928, 0.9927148, 0.99116534, 0.9509647, 0.01667753, 0.03557165, 0.0442868, 0.0495194, 0.98712665, 0.93445146, 0.031434886, 0.18558843, 0.12924424, 0.89878565, 0.15359457, 0.9923528, 0.972234, 0.3500966, 0.028920414, 0.956844, 0.1753763, 0.9854936, 0.7012442, 0.28377482, 0.058168627, 0.010418062, 0.98949426, 0.98078513, 0.31023765, 0.99098325, 0.98163337, 0.26478446, 0.9821453, 0.9825001, 0.985786, 0.97819626, 0.039613765, 0.9811499, 0.0641713, 0.013131809, 0.9894454, 0.98614025, 0.9827823, 0.9200159, 0.014296085, 0.019727437, 0.014816886, 0.024222866, 0.0319766, 0.1576709, 0.9853873, 0.9853709, 0.98984456, 0.98922104, 0.91304916, 0.62797266, 0.014102536, 0.9842228, 0.0136467805, 0.14770484, 0.060317747, 0.008556918, 0.99011225, 0.16854823, 0.050094742, 0.9862002, 0.99179196, 0.21880591, 0.18510209, 0.007586548, 0.010474018, 0.006745858, 0.082954206, 0.98913157, 0.96268976, 0.036240302, 0.9842302, 0.94191414, 0.9896171, 0.013862474, 0.05333961, 0.79402685, 0.0058526206, 0.020567402, 0.012245278, 0.9724821, 0.97254014, 0.023821851, 0.47668836, 0.98935705, 0.4252132, 0.08743374, 0.9909526, 0.9845244, 0.015340934, 0.98928386, 0.98994243, 0.984135, 0.04144613, 0.99047685, 0.98428416, 0.9687685, 0.038188666, 0.98716563, 0.092361085, 0.9789604, 0.974858, 0.9718231, 0.99121374, 0.98502827, 0.9566174, 0.02827764, 0.9905918, 0.9877751, 0.17128351, 0.9916932, 0.97703755, 0.9853617, 0.9849, 0.9426348, 0.9871387, 0.9830329, 0.0060798875, 0.97066027, 0.08590626, 0.01428337, 0.08852138, 0.2551566, 0.9793214, 0.97725177, 0.011531057, 0.024082445, 0.9656672, 0.99079645, 0.04725629, 0.9192529, 0.022199996, 0.06689256, 0.024457948, 0.00936166, 0.9168109, 0.98887384, 0.17698328, 0.978336, 0.9904833, 0.97375983, 0.9877352, 0.95157546, 0.9816519, 0.91569054, 0.020838166, 0.008473524, 0.9909677, 0.98782825, 0.02950272, 0.9861544, 0.010970671, 0.98883504, 0.93284947, 0.9832712, 0.9873444, 0.0075403336, 0.9897926, 0.007954891, 0.006857007, 0.9790589, 0.015805347, 0.9832772, 0.00867742, 0.027305307, 0.012835254, 0.9921383, 0.033819098, 0.9891446, 0.010783845, 0.036744874, 0.018474655, 0.19082738, 0.009877831, 0.92571265, 0.9899009, 0.98968154, 0.9644976, 0.9841069, 0.012050646, 0.008231737, 0.008810151, 0.050710697, 0.10280068, 0.97875607, 0.9834501, 0.8669216, 0.9913838, 0.9858431, 0.0066078296, 0.9754823, 0.037019264, 0.9552784, 0.009840341, 0.9922122, 0.12653446, 0.009039724, 0.1101641, 0.99098736, 0.02983336, 0.099139, 0.16808186, 0.08856215, 0.98975945, 0.017397137, 0.015099991, 0.98860544, 0.98272103, 0.98343235, 0.014270782, 0.075762376, 0.007512529, 0.97188276, 0.93843496, 0.40883142, 0.98106796, 0.99013597, 0.8164836, 0.15847962, 0.9902811, 0.98423463, 0.028203364, 0.65039855, 0.9919442, 0.009443955, 0.07580468, 0.08070131, 0.01918135, 0.016321652, 0.014758884, 0.3605589, 0.990686, 0.078073226, 0.799499, 0.9803359, 0.008234115, 0.22157155, 0.67039657, 0.009323476, 0.4177852, 0.98458374, 0.98201156, 0.9856425, 0.97329885, 0.9738599, 0.99047816, 0.129001, 0.989427, 0.07542572, 0.22225247, 0.36129937, 0.014723259, 0.06889172, 0.22907655, 0.99010605, 0.26115665, 0.9610269, 0.59492195, 0.44516152, 0.7942337, 0.018661914, 0.98864275, 0.017121399, 0.98630035, 0.88990694, 0.16317175, 0.98442656, 0.98921776, 0.9904862, 0.78836596, 0.071400374, 0.99006116, 0.9875492, 0.035059605, 0.016527077, 0.19822556, 0.99198556, 0.22810845, 0.014315684, 0.014945057, 0.9899564, 0.06966667, 0.03512662, 0.9895648, 0.021875732, 0.018133188, 0.020835588, 0.15230915, 0.0173148, 0.41785344, 0.0291049, 0.66945815, 0.9921864, 0.5513739, 0.978953, 0.022013512, 0.9641493, 0.96834904, 0.008550186, 0.99004376, 0.9897399, 0.07982245, 0.13024047, 0.96558696, 0.98653316, 0.01140759, 0.9918464, 0.015679494, 0.21660022, 0.043902777, 0.017743053, 0.1664111, 0.98167497, 0.98451567, 0.97968346, 0.016316878, 0.86610186, 0.04373913, 0.028978933, 0.08749905, 0.99024814, 0.022034463, 0.012836359, 0.98323375, 0.7549956, 0.058147, 0.9852615, 0.98216206, 0.6765947, 0.017225945, 0.18021353, 0.98037237, 0.9916841, 0.72676927, 0.11082282, 0.98077184, 0.007579472, 0.9709656, 0.106406845, 0.9918017, 0.9890646, 0.97917616, 0.9830034, 0.9892536, 0.4716712, 0.01062968, 0.012438303, 0.98034096, 0.79642665, 0.8185503, 0.016213903, 0.06398411, 0.97913307, 0.98888576, 0.9803801, 0.9891677, 0.07633584, 0.26429254, 0.9906681, 0.17248574, 0.1206462, 0.016975122, 0.98919123, 0.10919392, 0.07647572, 0.97413844, 0.029806452, 0.9795786, 0.60856193, 0.9664056, 0.6974179, 0.012187299, 0.006672436, 0.9386921, 0.8048655, 0.98643804, 0.98585004, 0.045918096, 0.4177884, 0.98877037, 0.8368966, 0.033402145, 0.44701046, 0.7610152, 0.02587572, 0.017889885, 0.9892848, 0.96923256, 0.013265009, 0.049352847, 0.043146532, 0.078498974, 0.9854779, 0.989075, 0.9349078, 0.039444685, 0.8679544, 0.01389483, 0.015692383, 0.35377803, 0.9581082, 0.2336305, 0.9902104, 0.9864942, 0.009248069, 0.06774994, 0.28989565, 0.0149251195, 0.018876432, 0.9163321, 0.015298961, 0.9801722, 0.9835036, 0.989347, 0.98056096, 0.12445541, 0.92369926, 0.0139928, 0.08507563, 0.9914697, 0.9871387, 0.010910095, 0.1116645, 0.053489897, 0.98834085, 0.39057976, 0.99082655, 0.20572338, 0.76894325, 0.16621658, 0.9898845, 0.01742841, 0.0766045, 0.008540817, 0.9906, 0.012839441, 0.98972464, 0.9909117, 0.008334849, 0.9881449, 0.4723832, 0.98630977, 0.06185722, 0.9866089, 0.006720895, 0.9869042, 0.97745466, 0.9547821, 0.05622669, 0.98969346, 0.3278717, 0.99101603, 0.9241128, 0.2591579, 0.06623265, 0.94326496, 0.053982925, 0.006876229, 0.057151437, 0.8402805, 0.018015373, 0.033231754, 0.98713183, 0.8159875, 0.018109454, 0.5551657, 0.057504002, 0.98034316, 0.82073474, 0.10675643, 0.99163264, 0.9092045, 0.975045, 0.03444871, 0.98680854, 0.99016273, 0.16024168, 0.98646706, 0.99222696, 0.67299795, 0.49510333, 0.41483736, 0.9837479, 0.9582624, 0.9621061, 0.07131095, 0.99063885, 0.035215948, 0.017856421, 0.991343, 0.97329456, 0.9852024, 0.74046814, 0.07628667, 0.03464752, 0.9816852, 0.9281282, 0.033162076, 0.015477766, 0.9870411, 0.7547371, 0.1977872, 0.017595401, 0.9169548, 0.4464359, 0.009092876, 0.00859134, 0.9847099, 0.93515396, 0.033985067, 0.98896605, 0.027577264, 0.10350227, 0.99042714, 0.966329, 0.9859861, 0.9905103, 0.15980285, 0.2261757, 0.3004087, 0.053873505, 0.98511744, 0.026950737, 0.9904783, 0.97896284, 0.011336093, 0.20363228, 0.967651, 0.98863995, 0.06955724, 0.9898198, 0.9870385, 0.98043764, 0.06955847, 0.96568274, 0.99190825, 0.98988336, 0.77307206, 0.90156686, 0.037965316, 0.9885993, 0.11587805, 0.0141165, 0.031855047, 0.99003863, 0.9830263, 0.05052746, 0.95187825, 0.55140346, 0.726503, 0.009691163, 0.91961545, 0.025814641, 0.16407394, 0.009958529, 0.03194355, 0.0071166866, 0.016383726, 0.02348924, 0.013620014, 0.011914595, 0.48650628, 0.3408864, 0.009442541, 0.5415658, 0.067419045, 0.99038917, 0.017222015, 0.97226566, 0.95131433, 0.96850735, 0.98957574, 0.48024118, 0.007380833, 0.9880409, 0.035445407, 0.31096923, 0.68954515, 0.35291925, 0.98288167, 0.99002165, 0.9857664, 0.94075936, 0.99056494, 0.052947603, 0.9842242, 0.9578046, 0.98851657, 0.9865143, 0.987587, 0.9892937, 0.047083933, 0.8922445, 0.022001892, 0.90185297, 0.9897474, 0.015625479, 0.04585159, 0.9888841, 0.9806716, 0.8383011, 0.04086702, 0.61841494, 0.16990733, 0.14930722, 0.99000496, 0.9450764, 0.9890549, 0.5005893, 0.027366662, 0.9852197, 0.014105946, 0.009267129, 0.23572032, 0.06469676, 0.8990068, 0.98802197, 0.9850759, 0.99090135, 0.99074554, 0.0083864, 0.98845685, 0.98791003, 0.9553741, 0.013111049, 0.58143646, 0.9892357, 0.988605, 0.1281076, 0.7618123, 0.9861676, 0.46060783, 0.037389338, 0.95706123, 0.050222665, 0.97128594, 0.98713195, 0.9886905, 0.04187449, 0.02715233, 0.200107, 0.99147415, 0.98341906, 0.005913675, 0.02167551, 0.93696046, 0.9830026, 0.9919785, 0.26421893, 0.014005851, 0.69507813, 0.45971286, 0.98577255, 0.9888488, 0.99184275, 0.925829, 0.0053473683, 0.065686345, 0.26884, 0.69820374, 0.20831093, 0.006079161, 0.9881996, 0.984484, 0.009856262, 0.97702277, 0.6233598, 0.9906573, 0.009592844, 0.67882675, 0.8261289, 0.94037545, 0.98994035, 0.8802295, 0.18556744, 0.02469372, 0.9125178, 0.37292224, 0.028447771, 0.007211546, 0.96822214, 0.01902707, 0.5733508, 0.9863738, 0.012728495, 0.49695784, 0.97547764, 0.026173526, 0.012116151, 0.021211619, 0.9901646, 0.9916408, 0.06863417, 0.9569821, 0.98662764, 0.059229113, 0.9738274, 0.080542296, 0.9907486, 0.031535216, 0.23180106, 0.85200495, 0.986358, 0.98971087, 0.98824304, 0.045657113, 0.0062764063, 0.030258456, 0.9892253, 0.983988, 0.98930347, 0.96791816, 0.01967866, 0.9463925, 0.9880977, 0.9797315, 0.9828074, 0.26917914, 0.28628615, 0.008966459, 0.9900602, 0.9879605, 0.97230387, 0.0173011, 0.007902206, 0.9906686, 0.010305814, 0.009607742, 0.005913832, 0.95543844, 0.009674511, 0.033842906, 0.98590547, 0.04005445, 0.01350262, 0.013818686, 0.017028213, 0.323551, 0.01809711, 0.123679794, 0.1931938, 0.03719098, 0.9913623, 0.88686323, 0.98720944, 0.89805645, 0.39318797, 0.012696681, 0.0077416273, 0.035611767, 0.040493067, 0.9382496, 0.98703194, 0.040085927, 0.01906003, 0.9771049, 0.016360387, 0.18443568, 0.9564079, 0.20050086, 0.98646367, 0.2980493, 0.12308213, 0.8083376, 0.77355444, 0.729779, 0.9823501, 0.9903818, 0.10754197, 0.03906159, 0.010619213, 0.8419187, 0.97851014, 0.050461676, 0.39514038, 0.98104364, 0.9725637, 0.9038448, 0.9835324, 0.98965377, 0.98481226, 0.68728906, 0.007763496, 0.7748334, 0.8266954, 0.9882867, 0.9694644, 0.98906916, 0.9424182, 0.9801784, 0.8658446, 0.970227, 0.827733, 0.31039038, 0.83239055, 0.9863434, 0.9880505, 0.0097380495, 0.9881775, 0.97231466, 0.96146005, 0.9902592, 0.99057466, 0.05232441, 0.99142444, 0.032528028, 0.9243527, 0.9806009, 0.9897473, 0.024545228, 0.9863949, 0.9889389, 0.98718137, 0.9433044, 0.049452037, 0.97694594, 0.21219686, 0.057928316, 0.032315698, 0.014253734, 0.97951365, 0.03337402, 0.014056745, 0.9846736, 0.9860771, 0.9897251, 0.9888461, 0.95644367, 0.037032712, 0.9696269, 0.012753747, 0.97621673, 0.0059443736, 0.99010414, 0.20471498, 0.023481086, 0.28278756, 0.48795837, 0.9749555, 0.98310894, 0.98046255, 0.007943981, 0.9620969, 0.01136473, 0.9908166, 0.98953676, 0.9890177, 0.029390357, 0.12698615, 0.80008155, 0.011604781, 0.41504174, 0.9903902, 0.029101279, 0.98572934, 0.98653877, 0.96604216, 0.5989753, 0.9856251, 0.010315663, 0.012659541, 0.99030346, 0.54744226, 0.99118716, 0.9777649, 0.01468832, 0.9332197, 0.9921272, 0.96932924, 0.97915053, 0.9887599, 0.9881456, 0.9830429, 0.46571672, 0.041284103, 0.17650898, 0.79627264, 0.21498264, 0.016248379, 0.9889714, 0.24818215, 0.6022714, 0.019827843, 0.98777956, 0.9909746, 0.6936655, 0.98481166, 0.06489873, 0.9855952, 0.052929685, 0.13863452, 0.9898426, 0.04631209, 0.0057442957, 0.47650415, 0.022604926, 0.82667714, 0.017340416, 0.98656905, 0.9894493, 0.023176027, 0.98108244, 0.020170672, 0.8918275, 0.0679453, 0.12600794, 0.98538995, 0.82317704, 0.9857113, 0.016931161, 0.02211359, 0.024552032, 0.020091362, 0.03952742, 0.51143575, 0.09713612, 0.033349168, 0.0205874, 0.14924148, 0.9907402, 0.2842667, 0.9842127, 0.011453848, 0.02622053, 0.013033206, 0.9866636, 0.017827928, 0.9891194, 0.01131799, 0.9860598, 0.5881481, 0.9610268, 0.011999518, 0.9854981, 0.98923373, 0.99079925, 0.98744094, 0.41869366, 0.015612158, 0.011007096, 0.017469833, 0.979673, 0.2564409, 0.045521393, 0.99059415, 0.98316634, 0.31762105, 0.19863057, 0.010794465, 0.013447972, 0.89998424, 0.03979067, 0.9781383, 0.98716646, 0.022180721, 0.16237499, 0.6775665, 0.9886081, 0.031106912, 0.019796342, 0.9673537, 0.017468099, 0.009942453, 0.9691337, 0.27565712, 0.9911375, 0.98626167, 0.09546329, 0.9724038, 0.92726403, 0.034712043, 0.92326933, 0.9904545, 0.012286572, 0.9926588, 0.23888952, 0.059593968, 0.93085, 0.027780164, 0.9917196, 0.9880045, 0.98386383, 0.99162966, 0.9905347, 0.98880064, 0.7962902, 0.050586764, 0.99045885, 0.8136793, 0.9766973, 0.9763323, 0.57076645, 0.16186588, 0.19577909, 0.012451888, 0.31487605, 0.07337434, 0.10461235, 0.85475886, 0.77249885, 0.23966528, 0.9893618, 0.96888596, 0.20342423, 0.9807421, 0.9899768, 0.80823123, 0.98878086, 0.98132986, 0.97213846, 0.9506096, 0.015665077, 0.23880704, 0.9840109, 0.97743785, 0.16960028, 0.6200414, 0.9823325, 0.9910052, 0.98774475, 0.015966006, 0.102378786, 0.9850279, 0.9883209, 0.10284228, 0.036687747, 0.028013675, 0.06947266, 0.7908884, 0.04294778, 0.9901376, 0.32386425, 0.012166641, 0.7976417, 0.23528859, 0.9896595, 0.9255045, 0.010614832, 0.9890086, 0.23247924, 0.035084277, 0.988485, 0.015764482, 0.897378, 0.96082425, 0.23811221, 0.10545666, 0.060615186, 0.9824853, 0.98745084, 0.018270062, 0.9836405, 0.76736915, 0.21699184, 0.020575957, 0.02078013, 0.98680824, 0.9857691, 0.9889332, 0.07428955, 0.9610199, 0.9911404, 0.01135418, 0.03761688, 0.04217125, 0.025154332, 0.010656181, 0.029696614, 0.6201093, 0.98702776, 0.9877495, 0.9836642, 0.37223157, 0.979552, 0.023728736, 0.98882324, 0.95106184, 0.10549525, 0.55509305, 0.2831085, 0.19718051, 0.03840543, 0.805476, 0.010907661, 0.15483099, 0.9759557, 0.97760665, 0.17339346, 0.9334989, 0.98862654, 0.015635984, 0.97995776, 0.99208516, 0.97856325, 0.88723826, 0.067201614, 0.015038348, 0.047615714, 0.98504794, 0.98488957, 0.92877096, 0.0135747725, 0.98560846, 0.050705273, 0.9678656, 0.006733333, 0.017236581, 0.9913901, 0.027641721, 0.8886089, 0.024358878, 0.30599037, 0.013871982, 0.05529441, 0.10798972, 0.932752, 0.98441535, 0.01571605, 0.033200555, 0.010169763, 0.9882131, 0.9304798, 0.052800246, 0.02305095, 0.990069, 0.016771913, 0.019306963, 0.012260934, 0.98667395, 0.70714945, 0.99059045, 0.98666674, 0.7761367, 0.016720407, 0.11094224, 0.37103787, 0.9836866, 0.02445173, 0.9863831, 0.97242683, 0.11492583, 0.034525674, 0.9707537, 0.99029666, 0.98693687, 0.96929777, 0.533226, 0.9549355, 0.71303135, 0.041737016, 0.695223, 0.010658825, 0.9909643, 0.012468751, 0.051814057, 0.95175266, 0.9313023, 0.04574272, 0.029894423, 0.6355693, 0.2929589, 0.99260193, 0.33610067, 0.9919559, 0.02465919, 0.98890775, 0.976689, 0.43330827, 0.79627097, 0.027941337, 0.89738023, 0.975947, 0.27547872, 0.98609537, 0.25974977, 0.99136883, 0.9541691, 0.04050803, 0.008772481, 0.10632212, 0.008287396, 0.97661227, 0.9808359, 0.98861665, 0.3120573, 0.93377614, 0.9856306, 0.92643404, 0.053296164, 0.009300469, 0.98898786, 0.1828136, 0.30857685, 0.9912116, 0.987633, 0.8708902, 0.019066192, 0.2518764, 0.98594505, 0.25660992, 0.9184086, 0.0063300324, 0.012873669, 0.07962738, 0.035866663, 0.99117684, 0.022260277, 0.056762386, 0.9888566, 0.9818856, 0.9911596, 0.025528928, 0.10142169, 0.86768234, 0.016326025, 0.20763886, 0.98649997, 0.042683095, 0.0490061, 0.9903796, 0.048746753, 0.99085534, 0.5708503, 0.99249035, 0.99113995, 0.96943206, 0.9917647, 0.043750603, 0.045885995, 0.20269585, 0.6400928, 0.16737099, 0.98049027, 0.0061590103, 0.93801004, 0.023759242, 0.0059320284, 0.01620735, 0.975983, 0.02503148, 0.008228311, 0.76563907, 0.93003345, 0.15088387, 0.013409846, 0.109799124, 0.95365554, 0.9777944, 0.99049145, 0.08353027, 0.9895041, 0.98494524, 0.013396183, 0.98456, 0.11088891, 0.1317571, 0.05191825, 0.030761538, 0.23551874, 0.98427963, 0.9718563, 0.98332673, 0.98803794, 0.018184861, 0.98876345, 0.97502655, 0.98254967, 0.05054231, 0.98281604, 0.45948744, 0.018216668, 0.02808857, 0.028643128, 0.043610316, 0.07324924, 0.701922, 0.04588394, 0.007820467, 0.01802003, 0.007847308, 0.014596803, 0.1232954, 0.13597822, 0.97534925, 0.9069483, 0.55034655, 0.9913197, 0.97665876, 0.9840072, 0.98429155, 0.94970334, 0.53895533, 0.9795257, 0.021347817, 0.84963423, 0.041031763, 0.97122675, 0.05111952, 0.024997346, 0.02070282, 0.096107475, 0.9139034, 0.032432664, 0.98246896, 0.22745901, 0.01645205, 0.29168323, 0.9888797, 0.9578137, 0.9903134, 0.8274351, 0.2098904, 0.017522769, 0.051376246, 0.766178, 0.9583699, 0.98074317, 0.7870941, 0.024584772, 0.9494805, 0.012450433, 0.7453484, 0.08377185, 0.9610582, 0.98856354, 0.21689592, 0.9749449, 0.0076304516, 0.93574464, 0.987207, 0.16934223, 0.1750023, 0.3036356, 0.03170523, 0.23149267, 0.027152536, 0.9657769, 0.9775554, 0.9773283, 0.9889051, 0.74312615, 0.028326258, 0.09970111, 0.44890165, 0.060235705, 0.9881246, 0.98284274, 0.010507736, 0.9838434, 0.98833305, 0.9903077, 0.045338348, 0.006331274, 0.9850697, 0.98937166, 0.014756326, 0.4375943, 0.14968681, 0.022435358, 0.041002676, 0.96535313, 0.2800156, 0.97718996, 0.9893784, 0.08693666, 0.11185247, 0.25652736, 0.040476814, 0.9389451, 0.01926147, 0.97852707, 0.9697936, 0.06399709, 0.99206454, 0.029624129, 0.9869299, 0.97706544, 0.12138224, 0.11835754, 0.12879948, 0.9901266, 0.4361112, 0.9908336, 0.031420346, 0.981383, 0.9890646, 0.5354173, 0.95199406, 0.10410507, 0.9901097, 0.99044037, 0.007787346, 0.047643315, 0.83690614, 0.13133183, 0.087020606, 0.7076498, 0.01567697, 0.013579858, 0.08238312, 0.24196313, 0.017772289, 0.015268913, 0.019380732, 0.98872817, 0.98757535, 0.97323674, 0.006726174, 0.7606774, 0.52683127, 0.99214923, 0.9845152, 0.16850246, 0.016337952, 0.9418442, 0.033864453, 0.9867921, 0.18876368, 0.8720175, 0.5015897, 0.38087624, 0.9884673, 0.07008731, 0.02033606, 0.2080293, 0.014243145, 0.06646451, 0.9870068, 0.9915622, 0.98972076, 0.39308316, 0.9891857, 0.9793671, 0.06567538, 0.030515904, 0.98711026, 0.060512498, 0.98856133, 0.9906784, 0.01621561, 0.87490165, 0.9530548, 0.9829343, 0.18732959, 0.9857194, 0.16887398, 0.98847556, 0.020160243, 0.010275259, 0.9912082, 0.98960775, 0.9890583, 0.97526884, 0.051382076, 0.013007395, 0.9082284, 0.006806635, 0.9923613, 0.9736979, 0.017649703, 0.99075425, 0.2956179, 0.013414958, 0.007838204, 0.053479165, 0.060702328, 0.02710543, 0.8662652, 0.93132997, 0.9912999, 0.029589806, 0.047294866, 0.9736864, 0.9911498, 0.2842724, 0.39096418, 0.0290971, 0.10737535, 0.034293473, 0.77201146, 0.99238986, 0.9733296, 0.9900379, 0.98136455, 0.013599863, 0.9887476, 0.03608996, 0.4702468, 0.9514625, 0.9676921, 0.9894769, 0.062685676, 0.15780023, 0.101793125, 0.48487267, 0.98468655, 0.85812396, 0.01318965, 0.91068727, 0.28133643, 0.008051325, 0.008840655, 0.97550756, 0.9831335, 0.98783547, 0.016869863, 0.037310187, 0.29639968, 0.9458262, 0.884499, 0.418023, 0.32231826, 0.98005545, 0.5747893, 0.97588885, 0.98084706, 0.9867395, 0.9315638, 0.008672746, 0.00923944, 0.32248932, 0.98483235, 0.015084087, 0.98827875, 0.016545067, 0.97913843, 0.95621014, 0.015204476, 0.46290335, 0.9911583, 0.02207969, 0.3692721, 0.04550195, 0.99128693, 0.024489105, 0.25884956, 0.009583647, 0.93527097, 0.9771031, 0.9215192, 0.9553077, 0.98658305, 0.020281628, 0.9819541, 0.030273577, 0.98454916, 0.984121, 0.020211564, 0.023888296, 0.8292892, 0.011877065, 0.9861422, 0.988884, 0.038467754, 0.9906702, 0.0114944875, 0.050936554, 0.014246842, 0.78071296, 0.209031, 0.98944455, 0.018644258, 0.19522913, 0.9874648, 0.072431855, 0.9900288, 0.98730755, 0.046989262, 0.010004631, 0.03483504, 0.25242385, 0.028534515, 0.982249, 0.1193759, 0.9790334, 0.5961335, 0.9855437, 0.7498794, 0.91398036, 0.9914117, 0.9883576, 0.98591214, 0.012776078, 0.007958354, 0.014555086, 0.18923156, 0.2605766, 0.7855945, 0.09427824, 0.9752676, 0.035702314, 0.29827556, 0.98064464, 0.0061199763, 0.016199535, 0.9861785, 0.35025916, 0.017767254, 0.985241, 0.03163304, 0.013702876, 0.98730963, 0.98285943, 0.2858686, 0.9635415, 0.036279757, 0.08122252, 0.014334442, 0.48642534, 0.29815403, 0.36740735, 0.9713561, 0.035954833, 0.044000894, 0.070252225, 0.92902374, 0.057684287, 0.02713807, 0.043457776, 0.8649551, 0.013716791, 0.068824984, 0.056802914, 0.9907259, 0.017176082, 0.362683, 0.98480886, 0.98029447, 0.011092133, 0.96150744, 0.014036057, 0.98357177, 0.98052865, 0.923826, 0.079764366, 0.9701342, 0.96955353, 0.0109823365, 0.033044904, 0.97618437, 0.94863784, 0.94447106, 0.015197538, 0.9872936, 0.010223757, 0.039237257, 0.9448567, 0.92158806, 0.009517674, 0.9453169, 0.04278564, 0.3621932, 0.021701926, 0.011949653, 0.70259035, 0.0139488755, 0.98094845, 0.012691756, 0.19269931, 0.9875287, 0.9907258, 0.9898477, 0.9071708, 0.9902328, 0.98787075, 0.9589041, 0.009244431, 0.98503083, 0.9830663, 0.3519776, 0.2934042, 0.04948568, 0.9715316, 0.0075181765, 0.9849989, 0.04247455, 0.78772277, 0.005964364, 0.9887311, 0.9330913, 0.9428774, 0.04862294, 0.9904779, 0.9691282, 0.9387319, 0.007478192, 0.98739177, 0.01539127, 0.010068828, 0.036734406, 0.98344177, 0.9799921, 0.020959398, 0.016344495, 0.18563122, 0.01222835, 0.98621297, 0.034630537, 0.991459, 0.02571329, 0.8884631, 0.037174273, 0.9884941, 0.21613578, 0.05376339, 0.016600667, 0.985388, 0.16038683, 0.05760883, 0.057537135, 0.1661666, 0.022076678, 0.9921703, 0.8550849, 0.14264362, 0.011242895, 0.07627802, 0.85626954, 0.027638951, 0.97148955, 0.05024191, 0.35430172, 0.08385697, 0.13903826, 0.007685502, 0.980452, 0.9019331, 0.97189593, 0.9704495, 0.21150969, 0.5599839, 0.9899692, 0.9416908, 0.9863317, 0.98952466, 0.011206887, 0.9918032, 0.030182749, 0.01976158, 0.00885352, 0.013936302, 0.09996086, 0.95155096, 0.12652081, 0.9578392, 0.012836262, 0.98585004, 0.015286065, 0.9877374, 0.15883024, 0.2689423, 0.032556582, 0.991392, 0.014372085, 0.4598256, 0.41045704, 0.2684649, 0.02802346, 0.9807084, 0.025526002, 0.2793043, 0.8632581, 0.007888901, 0.98989534, 0.98177767, 0.976127, 0.985524, 0.36838937, 0.12050757, 0.026572699, 0.09192487, 0.4876459, 0.9204052, 0.017672256, 0.031473625, 0.8661962, 0.33448428, 0.9905309, 0.989311, 0.33042243, 0.013001973, 0.019561997, 0.8259481, 0.90387636, 0.6485953, 0.017838325, 0.00846649, 0.99087244, 0.94863033, 0.8938263, 0.9767339, 0.68743855, 0.9401237, 0.05618588, 0.013993886, 0.009258352, 0.20535347, 0.100578286, 0.95189005, 0.9893483, 0.99159557, 0.24920282, 0.7685895, 0.98836446, 0.98629516, 0.70323235, 0.9755515, 0.32209185, 0.021853475, 0.007903836, 0.025705151, 0.99132514, 0.98649675, 0.9896569, 0.9785544, 0.9858435, 0.8937094, 0.9057817, 0.017146267, 0.992137, 0.60479724, 0.0114480825, 0.43996206, 0.2809043, 0.041560248, 0.55765665, 0.9889408, 0.0062942607, 0.006968985, 0.722628, 0.9766243, 0.0055237557, 0.6644239, 0.051087935, 0.19838095, 0.04414661, 0.015777037, 0.049425807, 0.25064272, 0.008094761, 0.97148806, 0.06665193, 0.99036855, 0.024162302, 0.057438295, 0.011492289, 0.9902867, 0.90279996, 0.17724775, 0.22470787, 0.04063395, 0.018148925, 0.8588485, 0.9286816, 0.98810554, 0.061350487, 0.987958, 0.94766486, 0.9814647, 0.9892936, 0.94655716, 0.9405891, 0.9745188, 0.9919872, 0.99105024, 0.5804769, 0.9917218, 0.98938483, 0.8794515, 0.9914507, 0.66839004, 0.9794409, 0.7887766, 0.006058696, 0.98242056, 0.028294403, 0.9844646, 0.9855876, 0.010742255, 0.0074130697, 0.96637994, 0.9646501, 0.990476, 0.8521751, 0.99023306, 0.9898031, 0.03412576, 0.14222884, 0.98964345, 0.019335363, 0.9669603, 0.8689119, 0.99088013, 0.011433824, 0.1380745, 0.9860598, 0.3418379, 0.98965466, 0.8621436, 0.99146336, 0.01678313, 0.13013351, 0.9149282, 0.279291, 0.121928185, 0.5849082, 0.6595541, 0.114675246, 0.43454027, 0.9871295, 0.98865825, 0.234921, 0.9919389, 0.17984886, 0.20407633, 0.017737377, 0.02434942, 0.8712385, 0.016475352, 0.9874527, 0.8884406, 0.014897333, 0.016217576, 0.9914562, 0.043831542, 0.039105304, 0.19107023, 0.013010574, 0.03711057, 0.08791494, 0.9913299, 0.9861944, 0.9889704, 0.045173842, 0.98972404, 0.99221367, 0.02505078, 0.9774215, 0.35032222, 0.9883243, 0.23737292, 0.018706618, 0.70101506, 0.9799392, 0.987497, 0.95697093, 0.9750206, 0.16835807, 0.95309496, 0.035784304, 0.89639944, 0.98621285, 0.86870676, 0.9869408, 0.62321264, 0.987751, 0.23784788, 0.037189197, 0.9911411, 0.50658464, 0.9924344, 0.9862531, 0.019886034, 0.10771219, 0.9905625, 0.01964707, 0.24122034, 0.031877395, 0.39156008, 0.02171415, 0.013503281, 0.008751445, 0.6569944, 0.08815017, 0.33674818, 0.014695691, 0.021759093, 0.9894232, 0.9897233, 0.98838544, 0.03197277, 0.03281278, 0.0062301517, 0.9887293, 0.98919046, 0.20133945, 0.9897883, 0.45840707, 0.029121883, 0.253438, 0.068400934, 0.023171071, 0.98633575, 0.88151073, 0.9287887, 0.9791119, 0.9921211, 0.87688625, 0.32467547, 0.9764857, 0.020396313, 0.01763715, 0.06897147, 0.010852476, 0.25427446, 0.016944103, 0.9689799, 0.036124036, 0.010373877, 0.2787404, 0.07013354, 0.97763944, 0.41066414, 0.9776012, 0.98976785, 0.18598272, 0.026070012, 0.8826373, 0.081366085, 0.9759838, 0.16468759, 0.98919255, 0.98740524, 0.99084586, 0.0059133666, 0.038582787, 0.98557705, 0.1210672, 0.9862365, 0.06887408, 0.9896147, 0.13499406, 0.91595846, 0.18804911, 0.04223951, 0.9689715, 0.01189288, 0.029358475, 0.97578657, 0.09029319, 0.081212014, 0.9872415, 0.991573, 0.01955603, 0.11671611, 0.9856753, 0.22137122, 0.11117763, 0.059043586, 0.95298135, 0.03293825, 0.08873215, 0.04358859, 0.98386836, 0.9242253, 0.036591917, 0.55745995, 0.86168087, 0.97501266, 0.012255352, 0.009558475, 0.027450722, 0.6901681, 0.09737531, 0.98658955, 0.9896264, 0.99058014, 0.012311816, 0.9826865, 0.97859293, 0.38788193, 0.9883449, 0.13756536, 0.961096, 0.97232777, 0.017926494, 0.97997653, 0.022621892, 0.031543996, 0.99160415, 0.014409893, 0.017708857, 0.014284042, 0.990767, 0.3351712, 0.9758553, 0.039648294, 0.72598636, 0.6669431, 0.98112375, 0.3221925, 0.9809559, 0.008894369, 0.018071245, 0.10068105, 0.1754853, 0.022903314, 0.7124371, 0.025493847, 0.008230412, 0.98525304, 0.017881459, 0.06399273, 0.9875014, 0.008269591, 0.04370347, 0.008333824, 0.8052503, 0.9873072, 0.061789416, 0.018144079, 0.023309043, 0.05169515, 0.98696846, 0.8151808, 0.08515323, 0.77183694, 0.18225537, 0.019126292, 0.96510935, 0.9687036, 0.13651067, 0.98695797, 0.014137046, 0.055291284, 0.9914629, 0.01127634, 0.011766171, 0.031571466, 0.97172934, 0.07395156, 0.99129444, 0.027258707, 0.5579682, 0.8121906, 0.028676648, 0.013838976, 0.011477369, 0.0102500385, 0.65047264, 0.010228857, 0.027626839, 0.77468294, 0.017525537, 0.95077956, 0.012940679, 0.021925021, 0.78631485, 0.31162146, 0.021866398, 0.4188166, 0.025781523, 0.08195614, 0.96810156, 0.75155795, 0.9835308, 0.0089929905, 0.011494662, 0.98783034, 0.9816814, 0.9903887, 0.00962679, 0.7235404, 0.007701148, 0.9184292, 0.979542, 0.105582446, 0.90996933, 0.02874465, 0.23657775, 0.9751754, 0.98990077, 0.9847055, 0.97320944, 0.4982459, 0.078734405, 0.020995658, 0.19859423, 0.013488729, 0.66279477, 0.98849535, 0.024395237, 0.007500827, 0.85368216, 0.9804515, 0.023585454, 0.14502333, 0.011077937, 0.0831687, 0.032299098, 0.14961146, 0.98875916, 0.68038315, 0.98713326, 0.5025642, 0.05137832, 0.04923305, 0.97256404, 0.9910211, 0.9479101, 0.094281994, 0.053026315, 0.020401347, 0.30795193, 0.9925447, 0.061008442, 0.3490486, 0.533772, 0.9883193, 0.028745087, 0.016257254, 0.76238877, 0.9776133, 0.99118596, 0.23095441, 0.016867671, 0.4642483, 0.52196294, 0.59388113, 0.17325285, 0.020149548, 0.87220705, 0.982064, 0.098729365, 0.9926859, 0.99057555, 0.9882061, 0.1616014, 0.05200232, 0.9371936, 0.014960811, 0.060431793, 0.060593825, 0.98419464, 0.98887235, 0.011914528, 0.9018997, 0.826346, 0.9861491, 0.9866527, 0.03238949, 0.015262218, 0.2746022, 0.009616861, 0.9887334, 0.04992429, 0.98664445, 0.978721, 0.98741686, 0.85947496, 0.10349435, 0.97497445, 0.063084684, 0.9751399, 0.9484955, 0.012373134, 0.036840975, 0.99166095, 0.98163855, 0.0153272115, 0.017268047, 0.19645676, 0.9891868, 0.82406, 0.98942345, 0.025778355, 0.014281887, 0.5870113, 0.026344283, 0.019545607, 0.9801805, 0.98397696, 0.057147283, 0.96804684, 0.9903522, 0.91476387, 0.9879511, 0.015800748, 0.9349966, 0.1799599, 0.21099287, 0.9910136, 0.019923436, 0.4150767, 0.7741799, 0.9890104, 0.060672037, 0.007357647, 0.5535357, 0.035589002, 0.9917424, 0.98812115, 0.98508966, 0.012570346, 0.44045907, 0.98953784, 0.9848151, 0.07766782, 0.8829312, 0.027714547, 0.98664296, 0.13060153, 0.9855128, 0.32414094, 0.9792336, 0.02166884, 0.98560846, 0.008796496, 0.98941785, 0.6050354, 0.050813697, 0.009589659, 0.13585602, 0.9856542, 0.79303527, 0.9899893, 0.025536217, 0.9692699, 0.9855642, 0.011279903, 0.97123057, 0.018678572, 0.9755077, 0.42723018, 0.98374987, 0.95162094, 0.9461445, 0.06884082, 0.9892784, 0.95611405, 0.22195561, 0.012656217, 0.9301508, 0.051129125, 0.009682599, 0.99154377, 0.5464302, 0.018331025, 0.8932926, 0.98682296, 0.9914995, 0.0629513, 0.9833134, 0.95605457, 0.9809483, 0.032538924, 0.98294246, 0.9922995, 0.20062898, 0.98249024, 0.009673084, 0.9872077, 0.045504738, 0.98903286, 0.011146646, 0.5644559, 0.9879231, 0.3823746, 0.6916554, 0.98756546, 0.979421, 0.053431135, 0.5841019, 0.24873236, 0.044979263, 0.057807676, 0.13303503, 0.026432708, 0.019682176, 0.35144544, 0.05505555, 0.09595626, 0.98800874, 0.99166065, 0.9901473, 0.88442737, 0.9910579, 0.92217344, 0.97979033, 0.7582504, 0.9886833, 0.9914772, 0.067915305, 0.09150376, 0.019648766, 0.7770014, 0.9875238, 0.97834134, 0.9911807, 0.9904541, 0.9866654, 0.91258913, 0.9747253, 0.20946321, 0.98371947, 0.979188, 0.52700824, 0.98933804, 0.8590053, 0.008443997, 0.012009081, 0.12683883, 0.9852423, 0.16028929, 0.98842967, 0.992284, 0.7714802, 0.040517837, 0.020792846, 0.031859305, 0.98750335, 0.008621304, 0.9881002, 0.9904845, 0.17489143, 0.39156008, 0.0076055056, 0.031909425, 0.26914862, 0.022746446, 0.98761046, 0.69685924, 0.981804, 0.14637442, 0.4278719, 0.01611861, 0.9908625, 0.98839915, 0.021093678, 0.44536707, 0.9912356, 0.011801893, 0.9883867, 0.98701537, 0.029757505, 0.433295, 0.98277044, 0.2745868, 0.020107364, 0.9404328, 0.28351986, 0.6038513, 0.98659956, 0.5289445, 0.9643061, 0.091127336, 0.011001247, 0.006635588, 0.9918739, 0.9878534, 0.9875239, 0.98539966, 0.26438418, 0.9849236, 0.9906346, 0.5520363, 0.9915092, 0.051459413, 0.95110834, 0.09781228, 0.7246535, 0.99038553, 0.19180779, 0.1380127, 0.038193956, 0.98870826, 0.013621982, 0.99193, 0.98975617, 0.5869145, 0.0867823, 0.9921341, 0.05161551, 0.984971, 0.96176934, 0.10097823, 0.033777896, 0.025460904, 0.9793164, 0.98561674, 0.9864237, 0.020109178, 0.05159773, 0.8002576, 0.0077608447, 0.9844088, 0.01662567, 0.010394213, 0.007994402, 0.9590231, 0.011660069, 0.945209, 0.9525075, 0.9887999, 0.9837562, 0.57220113, 0.99062496, 0.97542745, 0.9695869, 0.028213467, 0.93320215, 0.15066876, 0.1359299, 0.5195363, 0.025720142, 0.9885035, 0.986304, 0.99259275, 0.014585971, 0.12712221, 0.9912457, 0.0410317, 0.021314535, 0.9909733, 0.014497427, 0.94346523, 0.65264904, 0.98701525, 0.8356592, 0.013294676, 0.04446351, 0.69754744, 0.036377043, 0.93548185, 0.99125004, 0.044690743, 0.45416895, 0.9523214, 0.94449085, 0.98871624, 0.98476255, 0.71806407, 0.29770136, 0.96200705, 0.9830755, 0.5400312, 0.42224768, 0.0075365556, 0.9750735, 0.5083377, 0.9869817, 0.5410416, 0.9535264, 0.9830034, 0.013439264, 0.10301187, 0.9877579, 0.00861932, 0.012515562, 0.033747822, 0.9703101, 0.016787529, 0.026326014, 0.009499033, 0.9815161, 0.021196917, 0.9852683, 0.023069497, 0.9422928, 0.9714714, 0.1357244, 0.9895626, 0.43865258, 0.9760272, 0.29441437, 0.0687676, 0.12284498, 0.9909219, 0.24640237, 0.050822087, 0.9903054, 0.38817567, 0.9891242, 0.15616255, 0.9884525, 0.9217592, 0.038099155, 0.13448305, 0.021305868, 0.021842001, 0.0109853055, 0.12988204, 0.019113315, 0.987904, 0.12640359, 0.010896812, 0.9910081, 0.14861391, 0.11325316, 0.018832663, 0.069945514, 0.8159277, 0.10149393, 0.9907494, 0.04004301, 0.22689739, 0.017869648, 0.053875852, 0.4659207, 0.037887994, 0.98868257, 0.9830595, 0.04046776, 0.9893846, 0.9895752, 0.25314534, 0.99118876, 0.054753095, 0.98893505, 0.98151946, 0.041495774, 0.9874077, 0.14392346, 0.1606103, 0.98709273, 0.0313536, 0.06808343, 0.27532855, 0.013927798, 0.991954, 0.9911907, 0.06207376, 0.047976717, 0.018554373, 0.98675287, 0.047707237, 0.5328719, 0.8280628, 0.07941041, 0.5017383, 0.95482355, 0.020287532, 0.9904102, 0.9561317, 0.98561007, 0.9838885, 0.98470694, 0.009916174, 0.016913813, 0.036835272, 0.990284, 0.6538917, 0.079604745, 0.9839589, 0.1944504, 0.9318489, 0.17712247, 0.983392, 0.98319584, 0.98726916, 0.25246212, 0.03057128, 0.04018964, 0.19535138, 0.95757675, 0.85937935, 0.98656577, 0.009725711, 0.009416993, 0.008102624, 0.99189043, 0.21319355, 0.9882988, 0.9790637, 0.11467576, 0.9608683, 0.33754432, 0.7926689, 0.9889332, 0.07385039, 0.77484494, 0.020487005, 0.9688023, 0.54238033, 0.98337007, 0.01781645, 0.9914488, 0.98886955, 0.23467506, 0.116817154, 0.9898561, 0.05019397, 0.9436013, 0.05645129, 0.48665792, 0.03273269, 0.48295894, 0.99076426, 0.6371092, 0.3950339, 0.050168313, 0.94599426, 0.95700824, 0.068964414, 0.032725543, 0.07668576, 0.4719273, 0.98374176, 0.95072675, 0.0081361635, 0.033433754, 0.82908833, 0.97583187, 0.97815084, 0.16714229, 0.057562165, 0.9855483, 0.5331368, 0.30350822, 0.83468455, 0.73561996, 0.96123844, 0.93053705, 0.9870443, 0.9869413, 0.98378193, 0.021095129, 0.9882494, 0.022524318, 0.5391992, 0.9836391, 0.9884201, 0.91614264, 0.013727993, 0.01642354, 0.90910006, 0.96790856, 0.9761538, 0.98031336, 0.023042876, 0.9719578, 0.9672579, 0.8979804, 0.99103945, 0.9509668, 0.77718884, 0.9874341, 0.97325057, 0.98391867, 0.9894432, 0.8265435, 0.30030665, 0.9584033, 0.016131887, 0.9319755, 0.9790718, 0.97558707, 0.045875877, 0.030700339, 0.96772546, 0.98911244, 0.98501843, 0.022327168, 0.2980026, 0.8836623, 0.0091536455, 0.6818013, 0.44221896, 0.29309788, 0.011291485, 0.970708, 0.980528, 0.9894518, 0.9899832, 0.98773557, 0.9832277, 0.13133478, 0.02546239, 0.01890929, 0.98489136, 0.059212584, 0.06485359, 0.42171627, 0.046127927, 0.0323728, 0.09637781, 0.9900581, 0.42025107, 0.820448, 0.94905746, 0.24193892, 0.9902238, 0.013086615, 0.27798888, 0.95214814, 0.009267137, 0.08237018, 0.99119383, 0.99203956, 0.50976086, 0.9734234, 0.9735153, 0.98540246, 0.9872743, 0.016617456, 0.05207045, 0.9898679, 0.022973271, 0.017037673, 0.05696515, 0.99091196, 0.5019181, 0.9872591, 0.028315753, 0.023023047, 0.7792273, 0.98396367, 0.97996897, 0.016648296, 0.9908931, 0.98978513, 0.007557553, 0.9131458, 0.9786219, 0.5851067, 0.045287553, 0.41157857, 0.9900843, 0.009730758, 0.8817526, 0.13239893, 0.46291336, 0.9781335, 0.009839109, 0.9904287, 0.99098057, 0.68917686, 0.017952055, 0.12641828, 0.044354837, 0.98666114, 0.9874176, 0.041220706, 0.0445049, 0.39656386, 0.9586411, 0.97870433, 0.97078043, 0.9413025, 0.953563, 0.98678267, 0.9915707, 0.15681958, 0.97430986, 0.95042676, 0.99201304, 0.0119040245, 0.1144961, 0.14982367, 0.09010968, 0.034883354, 0.9259374, 0.024401631, 0.9723094, 0.9897736, 0.84333986, 0.97556746, 0.047060486, 0.9813596, 0.86947924, 0.009697306, 0.021674799, 0.008939906, 0.59075356, 0.94187236, 0.9322223, 0.8794383, 0.010675129, 0.010417644, 0.9429581, 0.20269048, 0.8958355, 0.014138384, 0.9811468, 0.9924291, 0.9712725, 0.9913958, 0.9894739, 0.9903937, 0.057385497, 0.99202955, 0.17494649, 0.9807895, 0.7888789, 0.039756022, 0.9873268, 0.97629845, 0.07684646, 0.91653264, 0.9898868, 0.7879709, 0.028813835, 0.8628386, 0.98519415, 0.08210443, 0.98371327, 0.12578605, 0.66704565, 0.5857501, 0.59747404, 0.042707946, 0.9908257, 0.63799226, 0.043575544, 0.98788714, 0.9841351, 0.228232, 0.37993973, 0.16292171, 0.89491785, 0.9670081, 0.96605086, 0.039749604, 0.31985798, 0.9852844, 0.015145195, 0.95014995, 0.9899984, 0.034080025, 0.14809912, 0.99025154, 0.10302892, 0.025790555, 0.34275928, 0.025775304, 0.016330427, 0.9837368, 0.01459747, 0.48359776, 0.98992425, 0.0069586304, 0.007535379, 0.84338707, 0.7945841, 0.035702217, 0.96261585, 0.021863963, 0.99054986, 0.9895923, 0.029957162, 0.9854938, 0.97065645, 0.773359, 0.01247268, 0.9892055, 0.97100157, 0.9328693, 0.9845494, 0.014493884, 0.2523809, 0.14687549, 0.19530967, 0.987785, 0.63661283, 0.15709132, 0.08402549, 0.1609151, 0.5448214, 0.14183275, 0.012781611, 0.013610666, 0.11989796, 0.991626, 0.06656495, 0.98095316, 0.029197816, 0.72132313, 0.84588206, 0.9747732, 0.9826062, 0.99144214, 0.9897613, 0.04376395, 0.01813884, 0.027661806, 0.56102955, 0.9893315, 0.49808034, 0.99120307, 0.9923504, 0.5151896, 0.9764988, 0.9831713, 0.967475, 0.9900995, 0.9797434, 0.9829501, 0.99032426, 0.98739177, 0.9892903, 0.9897119, 0.8152903, 0.019582495, 0.041087423, 0.043853525, 0.008972928, 0.19764233, 0.9903082, 0.9697383, 0.98957986, 0.8159054, 0.018609505, 0.9786633, 0.056256313, 0.9506986, 0.05541916, 0.023014288, 0.3095064, 0.75817245, 0.9883109, 0.012744959, 0.9917903, 0.9905839, 0.23290949, 0.09951075, 0.9903689, 0.9772599, 0.037475523, 0.98091495, 0.9890506, 0.9670817, 0.083382994, 0.13308026, 0.26350355, 0.01749392, 0.021107135, 0.9662243, 0.040819336, 0.45878872, 0.02082645, 0.010045444, 0.27463698, 0.9864835, 0.98598886, 0.10127323, 0.9923322, 0.9752999, 0.04499393, 0.06465902, 0.0110418135, 0.9610751, 0.98951715, 0.9783756, 0.42544073, 0.8532421, 0.0126659935, 0.99141073, 0.9892329, 0.089598104, 0.1642306, 0.053417012, 0.9890837, 0.042411823, 0.8239392, 0.9751457, 0.98788536, 0.009822934, 0.10855562, 0.03455305, 0.99130356, 0.13094355, 0.018260611, 0.95261073, 0.024993181, 0.98947865, 0.98890215, 0.97578037, 0.15526836, 0.90273947, 0.19729854, 0.99055326, 0.99060667, 0.012424059, 0.010049275, 0.035836548, 0.05888297, 0.019384297, 0.84664696, 0.9445756, 0.010548523, 0.98961616, 0.0065305103, 0.9909218, 0.42114848, 0.042412523, 0.98969704, 0.021440826, 0.033797555, 0.96362776, 0.9737835, 0.9664688, 0.9464906, 0.6747092, 0.024138987, 0.0626741, 0.035505712, 0.9700533, 0.96549726, 0.9892368, 0.9827608, 0.021145988, 0.97006917, 0.99236226, 0.02380825, 0.02517263, 0.80156845, 0.015912065, 0.9893974, 0.028366556, 0.10329927, 0.12844269, 0.9911287, 0.032931246, 0.036246084, 0.61297095, 0.9830688, 0.9361105, 0.16032952, 0.99226826, 0.08361553, 0.9061204, 0.19805767, 0.034551594, 0.98066986, 0.034709968, 0.98947585, 0.98031974, 0.101763465, 0.012196953, 0.054024078, 0.9665181, 0.986328, 0.054493267, 0.58415216, 0.87506014, 0.89779264, 0.097334, 0.97051966, 0.9894142, 0.9921583, 0.9918749, 0.9906122, 0.9896144, 0.00910037, 0.06782201, 0.9354956, 0.006594993, 0.99075246, 0.30162334, 0.9785498, 0.9796774, 0.98233485, 0.013204231, 0.9907135, 0.9829883, 0.9910319, 0.44789377, 0.02321612, 0.10212264, 0.98368555, 0.38574055, 0.9595408, 0.51895565, 0.044704672, 0.079615, 0.9852881, 0.8735818, 0.007094318, 0.9907293, 0.93211406, 0.99143034, 0.26329327, 0.99155444, 0.98772764, 0.021792652, 0.85899645, 0.9871818, 0.011893472, 0.03939722, 0.012871028, 0.091772795, 0.9886035, 0.98965365, 0.011016018, 0.03516317, 0.98495424, 0.9844785, 0.035442628, 0.04719676, 0.014533762, 0.9904203, 0.9886552, 0.9520919, 0.01532159, 0.018055487, 0.17078444, 0.97634447, 0.9888787, 0.9911901, 0.7643548, 0.07504109, 0.011217945, 0.990558, 0.9905363, 0.8978509, 0.9868624, 0.11438225, 0.44003484, 0.024474164, 0.9807012, 0.90694696, 0.014996112, 0.98850816, 0.9900388, 0.99004704, 0.7928047, 0.95925266, 0.98949206, 0.98985904, 0.026389796, 0.16195504, 0.01011073, 0.928768, 0.96319544, 0.012080464, 0.015815021, 0.98329306, 0.042207513, 0.56867284, 0.010565752, 0.28987065, 0.12467606, 0.015418846, 0.987129, 0.09014994, 0.02146303, 0.99093455, 0.01759824, 0.89575195, 0.98752856, 0.93035036, 0.9722304, 0.9856893, 0.4421394, 0.109641984, 0.9814559, 0.9874933, 0.99020886, 0.10781486, 0.98190284, 0.66407514, 0.9905861, 0.014401064, 0.9910856, 0.973724, 0.83971244, 0.0053618695, 0.036351226, 0.97028244, 0.78631324, 0.017262504, 0.022843946, 0.9345742, 0.9900129, 0.028440123, 0.47957388, 0.018701514, 0.98236394, 0.9608823, 0.9897607, 0.019091666, 0.014252232, 0.025559561, 0.9885461, 0.009222262, 0.5726069, 0.02959518, 0.054358162, 0.035074644, 0.022237917, 0.9909372, 0.0072897957, 0.28909504, 0.8963108, 0.45865247, 0.10265755, 0.9851105, 0.98732424, 0.18122253, 0.9811747, 0.9541224, 0.030488739, 0.01098715, 0.9917822, 0.0455494, 0.7297809, 0.9897161, 0.9898078, 0.017400147, 0.010681526, 0.0075468663, 0.8372308, 0.98453426, 0.018697638, 0.98684734, 0.9863028, 0.43438357, 0.98471177, 0.071372785, 0.95789105, 0.21874128, 0.17613809, 0.9738349, 0.9812686, 0.9856396, 0.98600113, 0.015717948, 0.114107445, 0.23095979, 0.070001796, 0.015733559, 0.007467635, 0.03963776, 0.98595077, 0.98837554, 0.9840253, 0.027457064, 0.39386448, 0.032761365, 0.05579907, 0.991998, 0.9735266, 0.006289589, 0.014827295, 0.02152768, 0.2209338, 0.023110172, 0.21447925, 0.9760827, 0.98352987, 0.9915439, 0.9290958, 0.047928, 0.78269017, 0.97600037, 0.054120783, 0.32484165, 0.14503156, 0.01701511, 0.01696723, 0.1518077, 0.9848201, 0.89087206, 0.99161106, 0.022386186, 0.14342575, 0.9810104, 0.99095964, 0.022846134, 0.10829242, 0.01104623, 0.27484643, 0.8631913, 0.24349885, 0.13139181, 0.69005394, 0.9791734, 0.5085167, 0.016646897, 0.7633748, 0.17074068, 0.018885015, 0.96470934, 0.77150476, 0.57150745, 0.54601634, 0.74426526, 0.07442014, 0.91123164, 0.28906325, 0.947661, 0.025253844, 0.96741885, 0.989174, 0.9690749, 0.009143946, 0.9885839, 0.53817767, 0.2235818, 0.025163122, 0.09768013, 0.845157, 0.02003341, 0.010665636, 0.9841614, 0.83593154, 0.9830021, 0.9852395, 0.010957324, 0.029750071, 0.99182135, 0.9868132, 0.98761255, 0.9788356, 0.97686267, 0.1610469, 0.008211658, 0.982832, 0.02799361, 0.9706095, 0.014332503, 0.9743692, 0.9586859, 0.48062843, 0.030103402, 0.010956905, 0.99048656, 0.017829657, 0.028726216, 0.052822847, 0.99103504, 0.9825065, 0.236017, 0.038224615, 0.024781976, 0.8896519, 0.98486507, 0.006838824, 0.99095625, 0.989219, 0.07561609, 0.9903195, 0.99185693, 0.06259923, 0.064282924, 0.9896436, 0.9898425, 0.029015727, 0.2852055, 0.9899747, 0.54411346, 0.07796717, 0.008663028, 0.963401, 0.9101528, 0.12197938, 0.0252095, 0.012877621, 0.96972376, 0.98699594, 0.9898789, 0.033484917, 0.85469496, 0.9901424, 0.9008401, 0.06256064, 0.012181456, 0.030701758, 0.73127115, 0.016808989, 0.98845845, 0.9878251, 0.13154726, 0.26429063, 0.26662946, 0.010816225, 0.96571136, 0.5355123, 0.010747495, 0.055207472, 0.22652692, 0.41509935, 0.9581629, 0.009911953, 0.016407503, 0.87083066, 0.965872, 0.95382386, 0.7237866, 0.10121743, 0.98960435, 0.05662482, 0.49691927, 0.8751297, 0.018303642, 0.012025489, 0.5051562, 0.98561805, 0.24956661, 0.98949814, 0.99162394, 0.03850265, 0.18298052, 0.9897147, 0.98437566, 0.9838815, 0.11952443, 0.0101282885, 0.9245241, 0.09274007, 0.11029996, 0.021166358, 0.9715973, 0.9727179, 0.98167855, 0.018742887, 0.8242532, 0.010291102, 0.028506756, 0.9841351, 0.04665878, 0.041375447, 0.012372633, 0.00667083, 0.08319402, 0.630838, 0.037157096, 0.90929484, 0.4892889, 0.032799162, 0.02177407, 0.013637079, 0.8420244, 0.30216286, 0.62880576, 0.025079934, 0.016246581, 0.018262202, 0.02574736, 0.76892555, 0.84599686, 0.02740309, 0.6601378, 0.14796956, 0.08700257, 0.9842652, 0.22036606, 0.9626648, 0.008767445, 0.03123393, 0.9902097, 0.06300738, 0.8583828, 0.9536574, 0.0144941015, 0.99074674, 0.1564558, 0.94393015, 0.036556665, 0.9867556, 0.31020132, 0.6129934, 0.035622194, 0.96607935, 0.027534949, 0.26812658, 0.18948504, 0.94717735, 0.98956126, 0.92772824, 0.9916413, 0.012065613, 0.014604899, 0.6192244, 0.9724531, 0.737757, 0.020905979, 0.98898673, 0.9172031, 0.074510336, 0.17060328, 0.018701619, 0.08287504, 0.096469246, 0.988403, 0.014611592, 0.8433962, 0.17226389, 0.029339, 0.6879072, 0.16674209, 0.0072813835, 0.6396, 0.981895, 0.98313016, 0.32142875, 0.04501693, 0.054350846, 0.017345153, 0.07623685, 0.011006894, 0.98992646, 0.29294217, 0.086810134, 0.1810023, 0.99185294, 0.9915249, 0.99214995, 0.991039, 0.039567217, 0.9608346, 0.36815307, 0.9878434, 0.15307896, 0.017586397, 0.93016875, 0.012370326, 0.97875834, 0.073342584, 0.9679358, 0.7730812, 0.99062705, 0.031705253, 0.93857163, 0.93151456, 0.012346143, 0.038846165, 0.052299898, 0.99063534, 0.9886608, 0.88128906, 0.06977159, 0.036088087, 0.9919333, 0.019001445, 0.15707281, 0.38901967, 0.08583441, 0.0094998535, 0.9838907, 0.670967, 0.9484373, 0.90318286, 0.10983408, 0.99018735, 0.009503418, 0.6965686, 0.98960096, 0.9893567, 0.989546, 0.017916465, 0.010582754, 0.04478446, 0.0863991, 0.95115685, 0.9773432, 0.3516329, 0.7032831, 0.011915314, 0.08930906, 0.21723753, 0.9321673, 0.9090756, 0.06359296, 0.24740049, 0.9802009, 0.97494125, 0.99134094, 0.9875422, 0.92112297, 0.025170617, 0.039688494, 0.9909305, 0.9817321, 0.47239593, 0.07878339, 0.0187292, 0.5661607, 0.98771024, 0.8432278, 0.009824901, 0.011421947, 0.9818933, 0.98683375, 0.02003555, 0.3862235, 0.17016953, 0.027562845, 0.98322195, 0.01664888, 0.11602144, 0.05883691, 0.22737262, 0.98796415, 0.9830064, 0.9613118, 0.640422, 0.19010852, 0.6420722, 0.96955216, 0.026690012, 0.9889769, 0.7301629, 0.284558, 0.9894107, 0.12049973, 0.16006649, 0.98431474, 0.989268, 0.9888974, 0.014899426, 0.012966626, 0.015570516, 0.066692166, 0.99218726, 0.9895466, 0.06166302, 0.10206388, 0.9899614, 0.2933196, 0.83210015, 0.98655534, 0.23177846, 0.019011494, 0.014020979, 0.09205936, 0.05140898, 0.089897074, 0.95459753, 0.020345142, 0.6829341, 0.013097236, 0.016369037, 0.0070211375, 0.97643, 0.3463143, 0.980288, 0.016094672, 0.98353267, 0.014560074]\n",
      "[1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "[1. 1. 0. ... 0. 1. 0.]\n",
      "3563 4000\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(targets)\n",
    "print(np.round(predictions))\n",
    "predictions_np = np.round(predictions)\n",
    "targets_np = np.array(targets)\n",
    "x, y = 0, 0\n",
    "for j in range(predictions_np.size):\n",
    "    if predictions_np[j] == targets_np[j]:\n",
    "        x += 1\n",
    "    y += 1\n",
    "\n",
    "print(x , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "10.924999999999997 | 0.9179216 0.11520854 | 0.9800835 0.0029032542 | 0.784819 0.14652695\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 89.08\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 6.03\n",
      "AUROC (%): 93.25\n",
      "\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 6.03\n",
      "AUROC (%): 93.25\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 10.92\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 99.95\n",
      "AUROC (%): 93.25\n",
      "\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 99.95\n",
      "AUROC (%): 93.25\n"
     ]
    }
   ],
   "source": [
    "print('IMDB Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_prob.numpy()), np.std(s_prob.numpy()), '|', np.mean(s_rp.numpy()), np.std(s_rp.numpy()), '|', np.mean(s_wp.numpy()), np.std(s_wp.numpy()))\n",
    "\n",
    "# Success Detection\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100 - err, 2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe.numpy(), risky.numpy())))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "# Prediction Prob: Right/Wrong classification distinction\n",
    "print('\\nPrediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp.numpy(), s_wp.numpy()\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "# Error Detection\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err, 2))\n",
    "safe, risky = -kl_r.numpy(), -kl_w.numpy()\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('\\nPrediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp.numpy(), -s_wp.numpy()\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cr_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example data (replace with your own)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X_train_texts \u001b[38;5;241m=\u001b[39m \u001b[43mcr_data\u001b[49m\n\u001b[0;32m     16\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m cr_labels  \u001b[38;5;66;03m# Labels\u001b[39;00m\n\u001b[0;32m     17\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Adjust according to your task\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cr_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare your data (X_train, Y_train, X_dev, Y_dev, etc.) and tokenize it\n",
    "def tokenize_text(text, tokenizer, max_length):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "# Example data (replace with your own)\n",
    "X_train_texts = cr_data\n",
    "Y_train = cr_labels  # Labels\n",
    "max_length = 200  # Adjust according to your task\n",
    "\n",
    "X_train_ids, X_train_masks = zip(*[tokenize_text(text, tokenizer, max_length) for text in X_train_texts])\n",
    "X_train_ids = torch.cat(X_train_ids, dim=0)\n",
    "X_train_masks = torch.cat(X_train_masks, dim=0)\n",
    "Y_train = torch.tensor(Y_train , dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr_data, cr_labels = load_data('./data/CR.test')\n",
    "print('Customer Reviews\\n')\n",
    "\n",
    "in_examples , out_examples =  X_test_texts , cr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 63/63 [00:28<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "batch_size = 8\n",
    "in_input_ids = tokenizer(in_examples, padding=True, truncation=True, return_tensors='pt')\n",
    "out_input_ids = tokenizer(out_examples, padding=True, truncation=True, return_tensors='pt')\n",
    "in_logits , out_logits = [], []\n",
    "# Determine the number of batches\n",
    "num_in_batches = (len(in_input_ids['input_ids']) + batch_size - 1) // batch_size\n",
    "num_out_batches = (len(out_input_ids['input_ids']) + batch_size - 1) // batch_size\n",
    "\n",
    "# Forward pass out-of-domain examples through the model\n",
    "for i in tqdm(range(num_out_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(out_input_ids['input_ids']))\n",
    "    batch_input_ids = out_input_ids['input_ids'][start_idx:end_idx]\n",
    "    batch_attention_mask = out_input_ids['attention_mask'][start_idx:end_idx]\n",
    "    with torch.no_grad():\n",
    "        batch_logits = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits\n",
    "    out_logits.append(batch_logits)\n",
    "out_logits = torch.cat(out_logits, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD Example Prediction Probability (mean, std):\n",
      "0.8660856 0.14170882\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 88.89\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 91.43\n",
      "AUROC (%): 61.22\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 91.43\n",
      "AUROC (%): 61.22\n",
      "Normality base rate (%): 87.69\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 1.34\n",
      "AUROC (%): 71.47\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn.metrics as sk\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "# Load the XLNet tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "def tokenize_sentences(sentences, max_length):\n",
    "    # Tokenize the sentences\n",
    "    tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Convert examples to PyTorch tensors\n",
    "in_tokenized = tokenize_sentences(in_examples, max_length)\n",
    "out_tokenized = tokenize_sentences(out_examples, max_length)\n",
    "\n",
    "# Calculate probabilities\n",
    "out_probs = torch.softmax(out_logits, dim=1)\n",
    "s_p_oos = torch.max(out_probs, dim=1)[0]\n",
    "s_all = torch.softmax(out_logits, dim=1)\n",
    "kl_oos = torch.log(torch.tensor(2.)) + torch.sum(s_all * torch.log(torch.abs(s_all) + 1e-10), dim=1, keepdim=True)\n",
    "\n",
    "# Print results\n",
    "print('OOD Example Prediction Probability (mean, std):')\n",
    "print(np.mean(s_p_oos.numpy()), np.std(s_p_oos.numpy()))\n",
    "\n",
    "print('\\nNormality Detection')\n",
    "print('Normality base rate (%):', round(100 * len(in_tokenized['input_ids']) / (len(out_tokenized['input_ids']) + len(in_tokenized['input_ids'])), 2))\n",
    "\n",
    "print('KL[p||u]: Normality Detection')\n",
    "safe, risky = kl_a.numpy(), kl_oos.numpy()\n",
    "\n",
    "if len(safe.shape) == 1:\n",
    "    safe = np.expand_dims(safe, axis=1)\n",
    "if len(risky.shape) == 1:\n",
    "    risky = np.expand_dims(risky, axis=1)\n",
    "\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Normality Detection')\n",
    "safe, risky = s_prob.numpy(), s_p_oos.numpy()\n",
    "\n",
    "if len(safe.shape) == 1:\n",
    "    safe = np.expand_dims(safe, axis=1)\n",
    "if len(risky.shape) == 1:\n",
    "    risky = np.expand_dims(risky, axis=1)\n",
    "\n",
    "\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Normality base rate (%):', round(100 * (1 - err / 100) * len(in_tokenized['input_ids']) / (len(out_tokenized['input_ids']) + (1 - err / 100) * len(in_tokenized['input_ids'])), 2))\n",
    "\n",
    "print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "safe, risky = kl_r.numpy(), kl_oos.numpy()\n",
    "\n",
    "if len(safe.shape) == 1:\n",
    "    safe = np.expand_dims(safe, axis=1)\n",
    "if len(risky.shape) == 1:\n",
    "    risky = np.expand_dims(risky, axis=1)\n",
    "\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100 * sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100 * sk.roc_auc_score(labels, examples), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
